{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Ques1) What is a Decision Tree, and how does it work?"
      ],
      "metadata": {
        "id": "edAcy0UgD-mA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. It represents decisions and their possible outcomes as a tree-like structure, where each internal node corresponds to a feature or attribute, each branch represents a decision rule, and each leaf node signifies the outcome or prediction. The tree is built by recursively splitting the dataset into subsets based on feature values that maximize the separation of classes (in classification) or minimize error (in regression). This is typically achieved using criteria like Gini Impurity, Information Gain, or Mean Squared Error. Decision Trees are intuitive, easy to interpret, and can handle both categorical and numerical data. However, they are prone to overfitting if not properly constrained through techniques like pruning or setting limits on tree depth."
      ],
      "metadata": {
        "id": "LrfAYutlEDtI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques2) What are impurity measures in Decision Trees?"
      ],
      "metadata": {
        "id": "Zi8MMG62EIzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Impurity measures in Decision Trees are metrics used to evaluate the quality of a split at each node by quantifying the disorder or impurity of the resulting subsets. They help determine which feature and threshold provide the best split of the data. Common impurity measures include:\n",
        "\n",
        "- Gini Impurity: Measures the likelihood of misclassifying a randomly chosen instance if it were randomly labeled according to the class distribution. A Gini Impurity of 0 indicates a pure node where all instances belong to a single class.\n",
        "\n",
        "- Entropy: Based on the concept from information theory, it measures the amount of information or uncertainty in the data. A pure node has an entropy of 0, while higher entropy indicates greater class diversity.\n",
        "\n",
        "- Variance Reduction (used in regression): Measures the decrease in variance after a split, aiming to create subsets that are as homogeneous as possible with respect to the target variable.\n",
        "\n",
        "- These measures guide the tree-building process, selecting the splits that reduce impurity and result in more homogenous subsets, ultimately improving the model's prediction accuracy."
      ],
      "metadata": {
        "id": "XfxNVTI3EMDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques3) What is the mathematical formula for Gini Impurity?"
      ],
      "metadata": {
        "id": "bHChXtWtEU6p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The mathematical formula for Gini Impurity at a node is given by:"
      ],
      "metadata": {
        "id": "8TmpkHJYEbth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Screenshot 2025-06-21 160917.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "YWY8X2orEhLp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here:\n",
        "ùê∫ is the Gini Impurity.\n",
        "C is the total number of classes.\n",
        "pùëñ is the proportion of instances belonging to class i at the node.\n",
        "\n",
        "The Gini Impurity measures the probability of incorrectly classifying a randomly chosen instance from the node if it were labeled according to the class distribution at that node. A lower Gini Impurity indicates a purer node, where most instances belong to a single class."
      ],
      "metadata": {
        "id": "wyUBgiSQEmkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques4) What is the mathematical formula for Entropy?"
      ],
      "metadata": {
        "id": "w8fifJ24E0UA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The mathematical formula for Entropy at a node is:"
      ],
      "metadata": {
        "id": "RXpTtNj9E4BG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Screenshot 2025-06-21 161102.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "FBvYivLRE71w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here:\n",
        "H is the Entropy.\n",
        "C is the total number of classes.\n",
        "pi is the proportion of instances belonging to class i at the node.\n",
        "\n",
        "Entropy quantifies the uncertainty or randomness in the class distribution. A pure node (where all instances belong to a single class) has an entropy of 0, while a node with evenly distributed classes has the highest entropy."
      ],
      "metadata": {
        "id": "JM7GDALkE_UE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques5) What is Information Gain, and how is it used in Decision Trees?"
      ],
      "metadata": {
        "id": "gAcVNfAqFIgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Information Gain is a metric used in Decision Trees to measure the improvement in purity achieved by splitting a dataset based on a specific feature. It quantifies the reduction in uncertainty or entropy after the dataset is divided into subsets. The formula for Information Gain (IG) is:"
      ],
      "metadata": {
        "id": "DFeacarLFLhZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Screenshot 2025-06-21 161234.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "IPpS40_vFSPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where:\n",
        "Hparent: Entropy of the parent node.\n",
        "\n",
        "n: Number of child subsets after the split.\n",
        "\n",
        "Sk: Subset k after the split.\n",
        "\n",
        "‚à£ùëÜùëò‚à£: Number of instances in subset\n",
        "\n",
        "‚à£S‚à£: Total number of instances in the parent node.\n",
        "\n",
        "‚à£ùêªùëò‚à£: Entropy of subset k.\n",
        "\n",
        "The algorithm evaluates possible splits and selects the one with the highest Information Gain, which indicates the greatest reduction in entropy. This process continues recursively, building a tree structure that maximizes purity at each node, thereby improving prediction accuracy."
      ],
      "metadata": {
        "id": "mBULt0umFVr3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques6) What is the difference between Gini Impurity and Entropy?"
      ],
      "metadata": {
        "id": "vuhaWpiiF6bA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Gini Impurity and Entropy are metrics used in Decision Trees to evaluate the quality of splits, but they differ in calculation and interpretation. Gini Impurity measures the probability of misclassifying a randomly chosen instance if it were labeled according to the class distribution,with its simpler formula avoiding logarithms. In contrast, Entropy, based on information theory, quantifies the disorder or uncertainty in the data using its formula making it slightly more computationally intensive.\n",
        "While Gini ranges from 0 (pure) to 0.5 (most impure for two classes), Entropy spans 0 (pure) to log2(ùê∂)(maximum uncertainty for C classes). Gini is computationally efficient and often preferred for speed, whereas Entropy may better handle nuanced class imbalances. Despite these differences, both metrics often yield similar results in practical applications, and the choice depends on specific data and preferences."
      ],
      "metadata": {
        "id": "7HdH4P8hGD5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques7) What is the mathematical explanation behind Decision Trees?"
      ],
      "metadata": {
        "id": "d_ZymZFjGrvG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The mathematical foundation of Decision Trees lies in recursively partitioning the dataset to minimize impurity and create homogenous subsets. At each node, the algorithm evaluates all possible splits for each feature by calculating impurity measures such as Gini Impurity or Entropy. For a given split, the Information Gain is computed as the reduction in impurity from the parent node to its child nodes. The split with the highest Information Gain is selected, dividing the dataset at that node. This process continues recursively until a stopping criterion is met, such as reaching a maximum tree depth, achieving pure nodes, or when further splits provide no significant gain. For regression tasks, impurity is measured using variance or mean squared error instead of Gini or Entropy. Decision Trees are optimized to maximize predictive accuracy by reducing impurity at each level."
      ],
      "metadata": {
        "id": "YhnRJtRkGvP_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques8) What is Pre-Pruning in Decision Trees?"
      ],
      "metadata": {
        "id": "a3nqJG7kG-bY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Pre-pruning in Decision Trees, also known as early stopping, is a technique used to prevent overfitting by halting the growth of the tree during its construction. Instead of allowing the tree to grow until it perfectly classifies all training data, pre-pruning imposes constraints on the tree-building process based on predefined conditions. These conditions may include limiting the maximum depth of the tree, requiring a minimum number of samples at a node to allow a split, or setting a threshold for the minimum Information Gain or impurity reduction needed for a split. By stopping the tree's growth early, pre-pruning reduces the model's complexity, ensuring it generalizes better to unseen data. However, overly aggressive pre-pruning can result in underfitting, where the tree fails to capture important patterns in the data. Balancing these constraints is crucial for optimal model performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5pxzgY9DHDr5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques9) What is Post-Pruning in Decision Trees?"
      ],
      "metadata": {
        "id": "K-fJXAJlHJx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Post-pruning, also known as pruning after construction, is a technique used in Decision Trees to improve generalization and prevent overfitting by reducing the complexity of a fully grown tree. After the tree has been constructed to its maximum depth, post-pruning involves systematically removing branches that provide little to no improvement in predictive accuracy on a validation set. The process typically evaluates the impact of pruning on a subtree by measuring metrics like classification accuracy or impurity reduction. If removing a branch results in a simpler tree with comparable or better performance on the validation data, the branch is pruned. Post-pruning ensures that the model is neither overly complex nor too simplistic, striking a balance between capturing meaningful patterns and avoiding noise. This method is generally more effective than pre-pruning, as it allows the tree to explore all potential splits before reducing its complexity.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lBFttSdiHNC2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques10)  What is the difference between Pre-Pruning and Post-Pruning?"
      ],
      "metadata": {
        "id": "Np42icSFHSs4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The key difference between Pre-pruning and Post-pruning lies in the stage of the Decision Tree construction at which pruning is applied. Pre-pruning, or early stopping, halts the tree's growth during its construction based on predefined criteria, such as maximum depth, minimum samples per node, or minimum impurity reduction. This prevents the tree from becoming overly complex, saving computational time but risking underfitting if important splits are prematurely excluded. In contrast, Post-pruning is applied after the tree has been fully grown. It involves removing branches that do not improve validation performance, creating a simpler model while retaining meaningful patterns. Post-pruning tends to be more effective, as it evaluates the entire tree before simplifying it, but it is computationally more expensive. Both methods aim to balance model complexity and generalization, with pre-pruning emphasizing efficiency and post-pruning focusing on accuracy.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JTQd1AROHWnz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques11) What is a Decision Tree Regressor?"
      ],
      "metadata": {
        "id": "WuCjl1YLHb8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A Decision Tree Regressor is a supervised machine learning algorithm used for regression tasks, where the goal is to predict a continuous target variable. It constructs a tree-like model by recursively splitting the dataset into subsets based on feature values that minimize the error in predicting the target variable. At each node, the algorithm selects the feature and threshold that result in the greatest reduction in variance or minimize the mean squared error (MSE) in the target variable for the resulting subsets. The splitting process continues until a stopping criterion is met, such as reaching a maximum depth or a minimum number of samples per node. The prediction for a given input is made by traversing the tree from the root to a leaf node, where the value of the leaf represents the predicted target (usually the mean of the target values in that leaf). Decision Tree Regressors are intuitive, can capture nonlinear relationships, and do not require feature scaling, but they are prone to overfitting and may require pruning or regularization techniques for better generalization."
      ],
      "metadata": {
        "id": "KqCcNUfSHfiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques12) What are the advantages and disadvantages of Decision Trees?"
      ],
      "metadata": {
        "id": "-9pg44PoHkWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Decision Trees are widely used in machine learning due to their simplicity and interpretability, but they also have notable limitations. Their advantages include being easy to understand and visualize, as they mimic human decision-making processes. They can handle both categorical and numerical data without requiring feature scaling or normalization. Decision Trees are flexible, capable of capturing nonlinear relationships, and work well with small to medium-sized datasets. However, their disadvantages include a tendency to overfit, especially with deep trees, which can lead to poor generalization on unseen data. They are sensitive to small variations in the data, which can result in significant changes in the tree structure. Additionally, Decision Trees can struggle with imbalanced datasets and are less effective when the relationships between variables are highly complex. Techniques like pruning, ensemble methods (e.g., Random Forests), or regularization are often used to address these limitations."
      ],
      "metadata": {
        "id": "0pqC5t4BHnav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques13) How does a Decision Tree handle missing values?-"
      ],
      "metadata": {
        "id": "IrSYGu9SHs8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Decision Trees handle missing values using various strategies, depending on the implementation. One common approach is splitting based on available data, where only instances without missing values for the feature being evaluated are used to determine the split. Some implementations use surrogate splits, which involve finding alternative features that closely mimic the behavior of the primary splitting feature to handle instances with missing values. Another method is imputation, where missing values are replaced with statistical estimates (e.g., mean, median, or mode) or predicted values before training the tree. Additionally, some Decision Tree algorithms assign missing values to all branches and weigh their contribution based on the proportion of samples in each branch. These strategies ensure that Decision Trees can work effectively even when the dataset contains missing values, although preprocessing or advanced techniques might still be needed for optimal performance."
      ],
      "metadata": {
        "id": "qHhCxABIHwhS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques14) How does a Decision Tree handle categorical features?"
      ],
      "metadata": {
        "id": "PVEnZ0IbH0kN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Decision Trees handle categorical features by evaluating splits based on the unique categories of the feature. For a categorical feature, the tree considers splitting the data by grouping the categories into subsets that maximize impurity reduction (e.g., Information Gain or Gini Impurity). This can involve binary splits, where categories are divided into two groups, or multiway splits, where each category becomes a separate branch. The choice of splitting depends on the algorithm implementation and the criteria used. For example, in binary Decision Trees, algorithms often evaluate all possible groupings of categories to find the optimal split. Decision Trees are inherently capable of handling categorical features without requiring one-hot encoding or other preprocessing, making them particularly flexible and intuitive for datasets with mixed data types.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SmXmvILPH3lH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques15) What are some real-world applications of Decision Trees?"
      ],
      "metadata": {
        "id": "D0b_XVYmH8nl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Decision Trees have a wide range of real-world applications across industries due to their simplicity and effectiveness. In healthcare, they are used for diagnosing diseases, predicting patient outcomes, and recommending treatments based on patient data. In finance, they assist in credit risk assessment, fraud detection, and investment decision-making. In marketing, Decision Trees help segment customers, predict customer churn, and personalize product recommendations. In manufacturing, they optimize quality control processes and predict equipment failures through fault detection. Additionally, in education, they support student performance analysis and resource allocation. Their interpretability makes them particularly valuable in scenarios where understanding the decision-making process is critical, such as legal and regulatory applications. Moreover, Decision Trees are often part of ensemble methods like Random Forests and Gradient Boosted Trees, which enhance their predictive power for large-scale and complex datasets."
      ],
      "metadata": {
        "id": "Tx3CxDo5H_Zn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRACTICAL QUESTIONS"
      ],
      "metadata": {
        "id": "ogGzCSkEIETQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques16) Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy?"
      ],
      "metadata": {
        "id": "VVLpI9U6IH8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "zz2DHdtHINul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques17) Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the\n",
        "feature importances?"
      ],
      "metadata": {
        "id": "gY-WesaaIVOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier with Gini Impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "id": "w1yN36JgIZTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques18)Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the\n",
        "model accuracy?"
      ],
      "metadata": {
        "id": "fncxU54pIfgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier with Entropy as the splitting criterion\n",
        "clf = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with Entropy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "7OnPRUKXIj0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques19)  Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean Squared Error (MSE)?"
      ],
      "metadata": {
        "id": "6JMsRjnCIpFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Train the regressor\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate and print the Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n"
      ],
      "metadata": {
        "id": "titX8jOdIsxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques20) Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz?"
      ],
      "metadata": {
        "id": "pyT1nsxHIxMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "import graphviz\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Export the Decision Tree to a DOT format\n",
        "dot_data = export_graphviz(\n",
        "    clf,\n",
        "    out_file=None,\n",
        "    feature_names=iris.feature_names,\n",
        "    class_names=iris.target_names,\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    special_characters=True,\n",
        ")\n",
        "\n",
        "# Visualize the tree using graphviz\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.render(\"iris_tree\")  # Save the visualization as a file (e.g., \"iris_tree.pdf\")\n",
        "graph.view()  # Open the visualization\n"
      ],
      "metadata": {
        "id": "B7QmIBDWI0lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques21) Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its accuracy with a fully grown tree?"
      ],
      "metadata": {
        "id": "ZsKCDD_eI4Yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier with a maximum depth of 3\n",
        "clf_depth_3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_depth_3.fit(X_train, y_train)\n",
        "y_pred_depth_3 = clf_depth_3.predict(X_test)\n",
        "accuracy_depth_3 = accuracy_score(y_test, y_pred_depth_3)\n",
        "\n",
        "# Train a fully grown Decision Tree Classifier\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print the accuracies\n",
        "print(f\"Accuracy with max depth of 3: {accuracy_depth_3:.2f}\")\n",
        "print(f\"Accuracy with fully grown tree: {accuracy_full:.2f}\")\n"
      ],
      "metadata": {
        "id": "9v6H8q8oI9lC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques22) Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its accuracy with a default tree?"
      ],
      "metadata": {
        "id": "3c71C7XgJB10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier with min_samples_split=5\n",
        "clf_min_samples_split = DecisionTreeClassifier(min_samples_split=5, random_state=42)\n",
        "clf_min_samples_split.fit(X_train, y_train)\n",
        "y_pred_min_samples_split = clf_min_samples_split.predict(X_test)\n",
        "accuracy_min_samples_split = accuracy_score(y_test, y_pred_min_samples_split)\n",
        "\n",
        "# Train a default Decision Tree Classifier\n",
        "clf_default = DecisionTreeClassifier(random_state=42)\n",
        "clf_default.fit(X_train, y_train)\n",
        "y_pred_default = clf_default.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "# Print the accuracies\n",
        "print(f\"Accuracy with min_samples_split=5: {accuracy_min_samples_split:.2f}\")\n",
        "print(f\"Accuracy with default tree: {accuracy_default:.2f}\")\n"
      ],
      "metadata": {
        "id": "5FiGdlIYJHKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques23) Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its accuracy with unscaled data?"
      ],
      "metadata": {
        "id": "MfglneAUJGYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Apply feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train a Decision Tree Classifier with scaled data\n",
        "clf_scaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Train a Decision Tree Classifier with unscaled data\n",
        "clf_unscaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = clf_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# Print the accuracies\n",
        "print(f\"Accuracy with scaled data: {accuracy_scaled:.2f}\")\n",
        "print(f\"Accuracy with unscaled data: {accuracy_unscaled:.2f}\")\n"
      ],
      "metadata": {
        "id": "DWL60SbvJPy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques24) Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass classification?"
      ],
      "metadata": {
        "id": "EoBL0PyIJUP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the One-vs-Rest strategy with a Decision Tree Classifier\n",
        "ovr_classifier = OneVsRestClassifier(DecisionTreeClassifier(random_state=42))\n",
        "\n",
        "# Train the classifier\n",
        "ovr_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ovr_classifier.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with One-vs-Rest: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "5cYS-BX2JTwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques25) Write a Python program to train a Decision Tree Classifier and display the feature importance scores?\n"
      ],
      "metadata": {
        "id": "ZREMNbLMJedK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Display feature importance scores\n",
        "print(\"Feature Importance Scores:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "id": "IbZytGKoJiId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques26) Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance with an unrestricted tree?"
      ],
      "metadata": {
        "id": "aloWjWw6JmAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Regressor with max_depth=5\n",
        "regressor_depth_5 = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "regressor_depth_5.fit(X_train, y_train)\n",
        "y_pred_depth_5 = regressor_depth_5.predict(X_test)\n",
        "mse_depth_5 = mean_squared_error(y_test, y_pred_depth_5)\n",
        "\n",
        "# Train an unrestricted Decision Tree Regressor\n",
        "regressor_full = DecisionTreeRegressor(random_state=42)\n",
        "regressor_full.fit(X_train, y_train)\n",
        "y_pred_full = regressor_full.predict(X_test)\n",
        "mse_full = mean_squared_error(y_test, y_pred_full)\n",
        "\n",
        "# Print the Mean Squared Errors\n",
        "print(f\"Mean Squared Error with max_depth=5: {mse_depth_5:.2f}\")\n",
        "print(f\"Mean Squared Error with unrestricted tree: {mse_full:.2f}\")\n"
      ],
      "metadata": {
        "id": "nC34ZsInJqXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques27)Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and visualize its effect on accuracy?"
      ],
      "metadata": {
        "id": "hgNaxBywJp9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier to find CCP alphas\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Get the effective alphas and corresponding total leaf impurities\n",
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas\n",
        "\n",
        "# Train classifiers for each alpha and record accuracy\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "for alpha in ccp_alphas:\n",
        "    clf_pruned = DecisionTreeClassifier(random_state=42, ccp_alpha=alpha)\n",
        "    clf_pruned.fit(X_train, y_train)\n",
        "\n",
        "    train_accuracies.append(clf_pruned.score(X_train, y_train))\n",
        "    test_accuracies.append(clf_pruned.score(X_test, y_test))\n",
        "\n",
        "# Plot the effect of CCP alpha on accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(ccp_alphas, train_accuracies, label=\"Train Accuracy\", marker=\"o\")\n",
        "plt.plot(ccp_alphas, test_accuracies, label=\"Test Accuracy\", marker=\"o\")\n",
        "plt.xlabel(\"CCP Alpha\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Effect of CCP Alpha on Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HDadT2lyJ12S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques28) Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision,Recall, and F1-Score?"
      ],
      "metadata": {
        "id": "mXl3OFcQJ640"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the performance\n",
        "precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
        "recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
        "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "\n",
        "# Print the detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
      ],
      "metadata": {
        "id": "GdIR5MkKJ-bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques29)Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn?"
      ],
      "metadata": {
        "id": "rpuBnbl9KC9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ffnwRTVGKHsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques30)  Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values for max_depth and min_samples_split?"
      ],
      "metadata": {
        "id": "RHf9TSAQKMPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    \"max_depth\": [3, 5, 10, None],\n",
        "    \"min_samples_split\": [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring=\"accuracy\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and the corresponding accuracy\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Test Set Accuracy with Best Model: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "cIhCOLhtKQT9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}